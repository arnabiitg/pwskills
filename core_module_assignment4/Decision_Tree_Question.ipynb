{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "61. What is a decision tree and how does it work?\n"
      ],
      "metadata": {
        "id": "bbfSbvcMl_lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A decision tree is a supervised machine learning algorithm that represents decisions and their possible consequences as a tree-like model. It works by recursively partitioning the data based on features to create a tree structure. Each internal node of the tree represents a feature or attribute, while each leaf node represents a class label or a prediction for regression tasks. Decision trees make predictions by traversing the tree from the root to a leaf node based on the values of the features."
      ],
      "metadata": {
        "id": "1QpmBR2Jl_qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "62. How do you make splits in a decision tree?"
      ],
      "metadata": {
        "id": "lF-UrIsJl_wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a decision tree, splits are made to partition the data based on different features at each level of the tree. The goal is to find the feature and its corresponding value that best separates the data points of different classes or minimizes the impurity measure. The splitting process continues recursively, creating branches and sub-branches until a stopping criterion is met, such as reaching a maximum tree depth or having a minimum number of samples per leaf."
      ],
      "metadata": {
        "id": "gSUrf7tFl_3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?"
      ],
      "metadata": {
        "id": "kUUe6t_Ol_-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Impurity measures, such as the Gini index and entropy, are used in decision trees to evaluate the homogeneity or impurity of a set of data points at a given node. The Gini index measures the probability of misclassifying a randomly chosen data point if it were randomly labeled according to the distribution of classes in the set. Entropy measures the average amount of information required to identify the class label of a randomly chosen data point. These measures help determine the best split that maximizes the homogeneity within the resulting subsets."
      ],
      "metadata": {
        "id": "ioklpBhNmAFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "64. Explain the concept of information gain in decision trees.\n"
      ],
      "metadata": {
        "id": "YDr2RxmxmALZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information gain is a concept used in decision trees to quantify the reduction in entropy or impurity achieved by splitting the data based on a specific feature. It measures the difference between the impurity of the parent node and the weighted average impurity of the child nodes after the split. Information gain is used to select the feature that provides the highest reduction in impurity, indicating the most informative feature for making predictions."
      ],
      "metadata": {
        "id": "bTOhZvlqmASO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "65. How do you handle missing values in decision trees?\n"
      ],
      "metadata": {
        "id": "jODFbZH1mAYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values in decision trees can be handled by various techniques:\n",
        "\n",
        "1. Assigning the most frequent value: Replace missing values with the most common value of the respective feature.\n",
        "2. Using surrogate splits: Create surrogate splits to handle missing values by considering alternative features that can act as proxies for the missing feature.\n",
        "3. Using missing value indicators: Treat missing values as a separate category or create a binary indicator variable to capture the absence or presence of missing values."
      ],
      "metadata": {
        "id": "XbqdRsFBmAfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "66. What is pruning in decision trees and why is it important?"
      ],
      "metadata": {
        "id": "a5Qf_IifmAlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruning in decision trees refers to the process of reducing the size of the tree by removing unnecessary branches and nodes. It helps prevent overfitting, where the tree becomes overly complex and fits the training data too closely, resulting in poor generalization to unseen data. Pruning is important to improve the model's ability to generalize by balancing the trade-off between complexity and simplicity."
      ],
      "metadata": {
        "id": "pN9lU4HJmArg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "67. What is the difference between a classification tree and a regression tree?\n"
      ],
      "metadata": {
        "id": "gaZrBh9amAyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A classification tree is used for categorical or discrete target variables. It assigns class labels to each leaf node, representing the predicted class for a given set of feature values. A regression tree, on the other hand, is used for continuous or numerical target variables. It assigns a predicted value to each leaf node, representing the average or majority value of the target variable for the corresponding data points"
      ],
      "metadata": {
        "id": "ccsCe2YMmA47"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "68. How do you interpret the decision boundaries in a decision tree?\n"
      ],
      "metadata": {
        "id": "SQFfcPLsmA_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision boundaries in a decision tree can be interpreted based on the splits and conditions applied at each internal node. Each split represents a decision criterion based on a feature and its corresponding value. The decision boundaries are created by these splits and define the regions in the feature space where different predictions or class labels are assigned. The traversal of the tree from the root to a leaf node determines the specific decision boundaries for individual data points"
      ],
      "metadata": {
        "id": "aozhvoTsmBFZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "69. What is the role of feature importance in decision trees?\n"
      ],
      "metadata": {
        "id": "cvt7vqEcmBLq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance in decision trees quantifies the relative importance of different features in making predictions. It measures the contribution of each feature to the overall reduction in impurity or information gain. Feature importance can be derived from the number of times a feature is used for splitting across all internal nodes or based on the reduction in impurity achieved by a feature. It helps identify the most relevant features and understand their influence on the decision-making process."
      ],
      "metadata": {
        "id": "z2FEUwa5mBRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "70. What are ensemble techniques and how are they related to decision trees?\n"
      ],
      "metadata": {
        "id": "Jbqo8ypnmBYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble techniques combine multiple individual models to create a more powerful and robust model. Decision trees are often used as base models in ensemble techniques such as random forests and gradient boosting. Random forests combine the predictions of multiple decision trees by averaging or voting, while each decision tree is trained on a randomly sampled subset of the data. Gradient boosting builds a sequence of decision trees, where each subsequent tree corrects the errors made by the previous trees, resulting in a more accurate model. Ensemble techniques leverage the diversity and aggregation of multiple decision trees to improve overall performance and reduce overfitting."
      ],
      "metadata": {
        "id": "yBetdI5nmBdy"
      }
    }
  ]
}